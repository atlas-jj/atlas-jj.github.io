<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Robot Learning Geometry</title>
  <meta name="description" content="Welcome! This site is under construction.
">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="stylesheet" href="/css/custom.css">
  <link rel="canonical" href="http://yourdomain.com/robot-learning-geometry/">
  <link rel="alternate" type="application/rss+xml" title="Blog" href="http://yourdomain.com/feed.xml">
  <!-- for mathjax support -->
    
      <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
        TeX: { equationNumbers: { autoNumber: "AMS" } }
        });
      </script>
      <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">Blog</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
        
          
          <a class="page-link" href="/about/">About</a>
          
        
          
        
          
        
          
        
          
          <a class="page-link" href="/robot-learning-geometry/">Robot Learning Geometry</a>
          
        
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="page-title-custom">Learning Geometry and Spatial Sense from Vision for Robotic Manipulation (Proposal)</h1>
  </header>

  <div class="post-content">
    <h6 class="page-author">Jun Jin &nbsp;   &nbsp;   &nbsp;   &nbsp;   &nbsp;  Feb 8, 2021</h6>

<!---
![GSD1 phenotype](/assets/images/chapter_1_intro.png)
$$X_{1}^{2}$$
some citations [[1]](#c1)
-->
<p><br />
<img src="/assets/images/cover_image.png" alt="GSD1 phenotype" /></p>

<p><br /></p>

<blockquote>
  <p>'’Learn how to see. Realize that everything connects to everything else.” <br /> — Leonardo da Vinci</p>
</blockquote>

<p><br /></p>

<h2 id="1-introduction">1. Introduction</h2>

<p>One characteristic of robotic task learning that differs from learning tasks in computer vision (CV) or 
natural language processing (NLP), is that defining “<em>what is the task</em>” per se is typically more 
difficult than task objectives like object detection or text classification.</p>

<p>In fact, we have very limited options in our toolbox to define “<em>what</em>”, 
including task function in early works <a href="#c1">[1]</a><a href="#c2">[2]</a>, reward function in reinforcement learning <a href="#c3">[3]</a><a href="#c4">[4]</a>, 
behavior cloning in imitation learning <a href="#c5">[5]</a>, 
and hierarchical approaches like inferring task goals in zero-shot imitation learning <a href="#c6">[6]</a>, task planning in
neural task programmers <a href="#c7">[7]</a> and so on <a href="#c8">[8]</a><a href="#c9">[9]</a><a href="#c10">[10]</a>. 
Unfortunately, none of these have a thorough study on their capability to define “<em>what</em>”, 
nor enough attention has been given to the problem of task specification.</p>

<p><img src="/assets/images/overview2.png" alt="overview2" class="center" style="width:90%;" /></p>

<p>The goal of the research presented is to use structured priors and representation learning to encode <em>what</em> is a task, 
with which a task can be generally parameterized, monitored and controlled. 
This problem, which we call <strong>robotic task representation learning</strong> that involves studying</p>
<ul>
  <li>(1) What structured priors should be introduced to represent a task that enables general task specification.</li>
  <li>(2) How a task-relevant representation should be structured
and learned from raw image inputs.</li>
  <li>(3) How such representation enables easier controller design, provides task monitoring.</li>
  <li>(4) How representational invariance further enables generalizable robot learning across different environments and categorical objects.</li>
</ul>

<p>For example, by introducing a geometry structured task prior, 
a screwing task’s representation encoding <script type="math/tex">Z_{t}</script> stays consistent regardless of types of screw driver or screws since the task can be invariantly 
defined as a relationship between tooltip and the screw top.</p>

<p><img src="/assets/images/human_task_specification.png" alt="human_task_specification" class="center" /></p>

<p>Human empirically has a good sense of “what is the task”, as studied in observational learning<a href="#c20">[20]</a> in psychology, 
the rapid grasp of task concept by simply watching adults or peers during a human infant’s cognitive developmental process is astonishing. 
Generally, there are two types of structures for task specification used by humans:</p>
<ul>
  <li>(1) A semantic structure that conceptually describes how a task should be organized, for example a recipe defines how to make a pizza.</li>
  <li>(2) A geometric structure that solidly defines spatial information of a task, for example a tenon assembly task without any nails.</li>
</ul>

<p>Similarly, the above two structures are applied in robotic research. The semantic approach is intensively studied in ontology and affordance based methods<a href="#c11">[11]</a> which describe a task in a tree of 
graph structure based on objects,  attributes and their functionalities. 
However, the mapping from semantic task concepts to robot actions without hand-engineered motion primitives remains challenging. 
The geometric approach<a href="#c12">[12]</a><a href="#c13">[13]</a>, as a comparison, defines a task by object spatial relationships, geometric constraints 
(point-coincidence, line-parallel, etc.) and their chaining to construct more complex tasks<a href="#c16">[16]</a>. 
Besides the advantage of task interpretability, it provides a way to monitor task execution and relates directly to 
robot control as studied in visual servoing<a href="#c19">[19]</a>. However, this approach is <strong>rarely studied</strong>.</p>

<h4 id="making-sense-of-vision-by-visual-feature-connections"><strong>Making Sense of Vision by Visual Feature Connections</strong></h4>

<p><img src="/assets/images/chapter_1_intro.png" alt="chapter_1_intro" class="center" /></p>

<p>Using geometry and spatial sense to define a manipulation task, is essentially using a 
predefined structure to make connections between objects and geometric features (object parts), 
in order to make sense of what has been seen. The above figure shows 17 examples of how 
geometry and spatial sense applies in various manipulation, gaming, autonomous driving 
and collision avoidance tasks.</p>

<p><strong>The theme of this research</strong> is to introduce geometry and spatial sense as a structured task representation in robot learning, for the purpose of improving 
state-of-art robotic task specification capabilities.</p>

<h2 id="2-learning-geometry-and-spatial-sense-from-vision">2. Learning Geometry and Spatial Sense from Vision</h2>

<p>So far, we’ve discussed general ideas of representing a robotic task based on geometry and spatial sense, which are essentially making feature connections as discussed above.
To ground our discussion, three questions arise:</p>
<ul>
  <li>(1): how to parameterize geometry and spatial sense in an end-to-end trainable manner that is scalable to encode any forms of visual feature connections;</li>
  <li>(2): how to relate such parameterization to a task definition in an unsupervised manner, without human annotated samples;</li>
  <li>(3): what benefits will such structured task representation bring to us, regarding learning efficiency, generalization, intepretability and so on.</li>
</ul>

<p><img src="/assets/images/Our_approach.png" alt="Our_approach" class="center" style="width: 90%;" />
In this research, we propose a series of unsupervised learning methods to directly learn task 
representation from 3rd-view human demonstration videos. Through examples of various manipulation tasks, we prove introducing geometry structure in robot learning will largely
reduce needed human demonstrations, improving learning efficiency, showing better generalization performance, and the learned geometric task repressentation provides decidability,
diagnosability and controllability.</p>

<p><br /></p>

<h2 id="3-learning-object-level-spatial-sense">3. Learning Object Level Spatial Sense</h2>

<p><img src="/assets/images/chapter3_overview.png" alt="chapter3_overview" class="center" /></p>

<p>Let’s begin by exploring how to learn object-level spatial sense from 3rd-view human demonstration videos. This work has been published as
<a href="https://ieeexplore.ieee.org/document/8793649" target="_blank"><em>“Robot Eye-hand Coordination Learning by Watching Human Demonstrations: a Task Function Approach”</em></a>, which appeared as a conference paper in 
<em>“2019 IEEE International Conference on Robotics and Automation (ICRA)”</em>.</p>

<p>The basic idea is quite simple: since the most significant source of observed pixel changes <script type="math/tex">(S_{t} \rightarrow S_{t+1})</script> is the spatial information variants
caused by human or robot action, is it possible to <strong>retrieve the spatial information from such a state change tuple <script type="math/tex">(S_{t}, S_{t+1})</script></strong>?</p>

<p><img src="/assets/images/chapter3_encoding_orderness.png" alt="chapter3_encoder_orderness" /></p>

<p>Suppose we observe an image state <script type="math/tex">S_{t}</script> at time t and apply action <script type="math/tex">a_{t}</script> which leads to the next image state <script type="math/tex">S_{t+1}</script>. 
Suppose a state change tuple is parameterized as <script type="math/tex">ds_{t} = f(S_{t}, S_{t+1})</script>, where <script type="math/tex">f</script> can be a simple two-state subtraction or a 
neural network. Let <script type="math/tex">\mathbf{R_{t}} \in \mathbb{R}^{d}</script> denote the spatial information retrieved by a task encoder function defined as:</p>

<script type="math/tex; mode=display">\mathbf{R_{t}} = {E}_{\mathbf{\theta}}(ds_{t}|a_{t})</script>

<!---
, where $$\mathbf{\theta}$$ are the parameters of task encoder function, and $$a_{t}$$ is a _generic action_ which defines any actions that will cause state change $$ds_{t}$$ regardless of human or robot specific actions. 
This definition of $$a_{t}$$ circumvents the _correspondence problem_ as studied in learning from demonstration (LfD) literature~\cite{argall2009survey} 
without knowing a specific action format since we directly study motion outcomes across a human demonstrator and a robot imitator. 
Then $$a_{t}$$ is implicitly represented in $$ds_{t}$$ thus can be removed in later optimization. 
--->
<p>Now the problem becomes: how to optimize 
<script type="math/tex">{E}_{\theta}</script> given a set of 3rd-view human demonstration videos so that its output <script type="math/tex">\mathbf{R_{t}}</script> defines the task? 
Our solution is the introduced <strong>“Expert Orderness Assumption”</strong> that: 
<code class="highlighter-rouge">given a sequence of human demosntration video frames, the temporal order of frame transitions near-optimally defines
the task.</code> This assumption guides the unsupervised learning from human demonstration videos, which is proposed as <em>InMaxEntIRL</em>.
<!---
The term "near-optimally" refers to the fact that not all human demonstrators are perfect expert, which is similar to imperfect expert demonstration issues as studied in
inverse reinforcement learning (IRL) literature. So 
---></p>

<p><a href="https://www.youtube.com/watch?v=KXQbUPw4iw0&amp;ab_channel=JunJin" target="_black"><img src="/assets/images/ICRA2019_video_play.png" class="center" /></a></p>

<p>So how it works? In the above stacking blocks task, results show that learning a spatial task encoding enables a moderate success rate given only a small number (1~10) of
human demonstration video samples, and exhibits strong generalization performance regarding task/environment changes.</p>

<p><img src="/assets/images/chapter3_vis_encoding.png" alt="chapter3_vis_encoding" class="center" style="width:80%;" /></p>

<p>Lastly, we examine what exactly has the spatial task encoder learned. We visualize the learned spatial task encoding by collecting samples from a human 
kinesthetic teaching process, then visualize the norm of spatial task encoding output <script type="math/tex">R_{t}</script> using a color map. The figure above shows the colored sphere indicating
a tendency towards the 3D target.</p>

<p><img src="/assets/images/system_overview3.png" alt="system_overview3" class="center" />
The above figure shows a systematical view of how a spatial task encoder is learned and how to apply the output <script type="math/tex">R_{t}</script> in a closed-loop controller.</p>

<h2 id="4-learning-geometry-skills">4. Learning Geometry Skills</h2>

<p>Successively, we explore further from learning object-level spatial sense to a more fine-grained task representation framework—using geometric constraints, 
i.e., connections between geometric features / primitives, which we refer as geometry skills. This approach of robotic task representation was originally studied in 
visual servoing litetratures<a href="#c12">[12]</a><a href="#c13">[13]</a><a href="#c14">[14]</a><a href="#c15">[15]</a> which rely on human hand selected features. We propose to directlyl learn geometric feature constraints as a task encoding, and based on which, 
to learn an optimal selector instead of a hand engineered one.</p>

<p><img src="/assets/images/chapter4_overview_small.png" alt="chapter4_overview_small" class="center" /></p>

<p>This work has been published as two papers: <a href="https://ieeexplore.ieee.org/document/9196570" target="_blank">
<em>“Visual Geometric Skill Inference by Watching Human Demonstration”</em></a> appeared as a conference paper in 
<em>“2020 IEEE International Conference on Robotics and Automation (ICRA)”</em>, and <a href="https://ieeexplore.ieee.org/document/9196570" target="_blank">
<em>“A Geometric Perspective on Visual Imitation Learning”</em></a> in <em>“2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)”</em>.</p>

<!---
Reasons are three folds: (1) An optimal feature connection selector can select feature connections dirrectly 
 from $$\mathcal{F}$$, thus avoiding human hand selection and feature tracking; (2)  Encoding feature connections directly can utilize the rich representation power of deep neural networks;
(3) Encoding feature connections directly can be seen as learning a task representation. It provides the possibility to learn invariant task representations that stay consistent during task execution, 
across environmental settings and categorical objects. We will explore that in the next section.
--->

<!---
This approach of manipulation task representation is based on two assumptions:
(1) a task that can be defined by connections between geometric features on robot end feectors, tools and objects; (2) and all the features can be simutaneously observed in the same camera system.
The above assumption, though limits its applicability, can still cover a wide range of manipulation tasks, for example: non-prehensible tasks --- command the manipulator reaching a target
pose or changing object states, and prehensible tasks --- command the manipulator using tools to change object states.
--->

<p><img src="/assets/images/chapter4_task_relevance_selector.png" alt="chapter4_task_relevance_selector" class="center" /></p>

<p>In our approach, feature connections are represented as vectors <script type="math/tex">Z_{i}</script> by an encoder function <script type="math/tex">E_{k}</script>. And a selector <script type="math/tex">U_{k}</script> selects
feature connections based on their task-relevance probability, which is computed from <script type="math/tex">Z_{i}</script>. As a result, it forms a differentiable stream from feature connections
to their task-relevance probability. Now the question is how to parameterize an encoder <script type="math/tex">E_{k}</script>?</p>

<p><img src="/assets/images/chapter4_two_graph_types.png" alt="chapter4_two_graph_types" /></p>

<p><script type="math/tex">E_{k}</script> is essentially a relationship encoder. We propose two types of parameterzation methods based on graph neural networks (GCN). The reasons why using a GCN are: 
 (1) it’s <strong>permutation-invariant</strong> that the output <script type="math/tex">Z</script> is invariant to input
feature orders; (2) it’s <strong>scalable</strong> that can define complex relationships. How should we optimize the encoder and selector given a human demonstration video?</p>

<p><img src="/assets/images/chapter4_InMaxEntIRL.png" alt="chapter4_InMaxEntIRL" class="center" /></p>

<p>To optimize the encoder <script type="math/tex">E_{k}</script> and selector <script type="math/tex">U_{k}</script>, we continue to use the <em>“expert orderness assumption”</em>
with a changed <script type="math/tex">r_{t}</script> definition based on geometric loss computed from geometric constraints. 
 <!---
Type __A__ are feature descriptor based graphs which rely on hand crafted feature descriptors. 
However it limits its applicability since efficient feature descriptors for complex geometric primitives (line, conics) are difficult to design. Type __B__ removes this limitation by
using generalized image patch based graphs, which take a fixed dimension of image patches as input and utilize the rich representation power of deep neeural networks. As a results, deep
feature are learned as the encoder being optimized.
---></p>

<p><a href="https://www.youtube.com/watch?v=NnZM5ZsKc1s&amp;ab_channel=JunJin" target="_black"><img src="/assets/images/learning_geometry_cover.png" class="center" /></a></p>

<p>So how does it work? We show that, instead of learning actions from image pixels, learning a geometry-parameterized task concept provides an explainable and invariant representation across
demonstrator to imitator under various environmental settings.</p>

<!---
<img src= "/assets/images/chapter4_five_tasks.jpg" alt="chapter4_five_tasks" class="center"/>
<img src= "/assets/images/chapter4_human_robot.png" alt="chapter4_human_robot" class="center"/>
<img src= "/assets/images/chapter4_generalization_results_small-min.png" alt="chapter4_generalization_results_small-min" class="center"/>
[chapter4_generalization_setup](/assets/images/chapter4_generalization_setup.jpg)
[chapter4_generalization_results](/assets/images/chapter4_generalization_results.png)
The table above shows evaluation results under environmental / task setting changes. Compared to a hand-feature selection and tracking (hand-tracking) method,
this approach learns an optimal selector that directly selects feature connections based on their task relevance.
-->

<p><img src="/assets/images/fig8_IROS.png" alt="fig8_IROS" class="center" /></p>

<p>Lastly, we examine what has been optimized given a human demonstration video. We save snapshots of the learned encoder model at three different training stages: S1, S2, and S3 
represent the early, middle and final training stages respectively. Then the same robot video is fed into the three model snapthos and outputs the time-series geometric loss
from the selected top feature connections. To do this, we had the robot perform the task via teleoperation, then record video frames. Results show that we are actually
optimizing the encoder, selector to produce “good” control signals.</p>

<h2 id="5-learning-invariant-task-descriptors">5. Learning Invariant Task Descriptors</h2>

<p><img src="/assets/images/chapter5_invariant_geometry_small.png" alt="chapter5_invariant_geometry_small" /></p>

<p>In this section, we keep exploring how a geometry-structured task encoding <script type="math/tex">Z</script> will <strong>generalize across tasks with categorical objects</strong>. The term “categorical” 
means objects share the same functional parts that relate to a task definition. For example, in an open-bottle-cap task, the cap of various bottles is the shared functional parts,
and the task definition stays the same by relating gripper fingers to the caps. What insights can this bring to us?</p>

<p>Commonly, a task has two parts: a <em>“what”</em> specifies the task and a <em>“how”</em> drives task execution. 
One interesting characteristic of a manipulation tasks’s <em>“what”</em> is its consistency in the time domain that it won’t change during task execution, 
consistency across different task executors that it won’t change from human to robot, consistency in object categories that it won’t change because of different textured objects, 
consistency in different environmental settings that it won’t change given different backgrounds, illuminations and initial conditions. We call it as <strong>“Task Specification Invariance”</strong>.</p>

<p><img src="/assets/images/TD_task_equivalence.png" alt="TD_task_equivalence" class="center" /></p>

<p>As illustrated above, the task is to land a cart on a slope at specific positions as defined by two point-coincidences. 
Along the time domain when the cart moves, its positions will change, however the task definition specified by two pairs of point connections 
stays consistent (A). Likewise, the background may change to pink(B), the cart may change to orange color(C) or a smaller size(D), 
however the task definition stays invariant.</p>

<p><img src="/assets/images/chapter5_real_tasks.png" alt="chapter5_real_tasks" class="center" style="width:90%;" /></p>

<p>When looking from a geometry perspective, it means that the relational parts of objects form an invariant task 
representation. For example (A), the screw driver’s functional part is always a pen tip regardless of what size, color 
of a screw driver it is. The screwing task is always the relationship between its pen tip to the screw top, 
and its vertical direction to the screw body. For another example (B), the book organizing task needs always look at the relationship
of a book edge to the vertical line of book piles, regardless of what kind of book (size, texture) it is. 
To this end, readers probably realized the relationship in geometry feature level between object-robot gripper, 
object-object is the key for manipulation task representation.</p>

<p>As a result, representing a manipulation task using relational parts of the objects brings the insight of invariant task representation 
which is crucial for robot learning in order to generalize well. We propose to introduce geometry as inductive bias to learn an invariant task representations.
Specifically, given 3rd-view human demonstration videos performing the same task with different objects,  we aim to optimize a graph structured encoder and selector
that selects connections between functional parts of objects, as an invariant task representation.</p>

<p><img src="/assets/images/chapter5_optimization.png" alt="chapter5_optimization" class="center" /></p>

<p>We propose an approach utilizing the <strong>“Expert Orderness Assumption on Categorial Objects”</strong> defined as: Given several human demonstration videos of performing 
the same task but with categorical objects:</p>
<ul>
  <li>(1) the temporal order of frame transitions of each video near-optimally defines the task,</li>
  <li>(2) the task descriptor <script type="math/tex">\mathbf{Z}^{\mathbb{G}}_{i}</script> of each video stays consistent, where <script type="math/tex">\mathbb{G}</script> defines the prior geometry structure.</li>
</ul>

<p><img src="/assets/images/sapien.png" alt="sapien" /></p>

<p>To evaluate our proposed method, we design a table organization task in a Sapien simulator<a href="#c17">[17]</a>. The task is to organize messy items on a table in a neat way. It was 
firstly introduced in IROS 2020 as a manipulation challenge called “Open Cloud Robot Table Organization Challenge” (OCRTOC)<a href="#c18">[18]</a>. In OCRTOC, an organization task is defined by specifying 
each object’s target pose w.r.t. the table, which is quite cumbersome. In this work, we change it to a more natural setting: human demonstrate the organization task using different object
instances, robot is requried to learn the task that generalizes to categorical objects. Under this new setting, the robot is required to organize books, mugs in a neat way 
regardless of what books or mugs it is manipulating. As a result, it’s suitable for the study of categorical object generalization.</p>

<h2 id="references">References</h2>
<p><a id="c1">[1]</a> 
Samson, Claude, Bernard Espiau, and Michel Le Borgne. Robot control: the task function approach. Oxford University Press, Inc., 1991.</p>

<p><a id="c2">[2]</a> 
Samson, C., and B. Espiau. “Application of the task-function approach to sensor-based control of robot manipulators.” IFAC Proceedings Volumes 23.8 (1990): 269-274.</p>

<p><a id="c3">[3]</a>
Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018.</p>

<p><a id="c4">[4]</a>
Levine, Sergey, et al. “End-to-end training of deep visuomotor policies.” The Journal of Machine Learning Research 17.1 (2016): 1334-1373.</p>

<p><a id="c5">[5]</a>
Argall, Brenna D., et al. “A survey of robot learning from demonstration.” Robotics and autonomous systems 57.5 (2009): 469-483.</p>

<p><a id="c6">[6]</a>
Pathak, Deepak, et al. “Zero-shot visual imitation.” Proceedings of the IEEE conference on computer vision and pattern recognition workshops. 2018.</p>

<p><a id="c7">[7]</a>
Xu, Danfei, et al. “Neural task programming: Learning to generalize across hierarchical tasks.” 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2018.</p>

<p><a id="c8">[8]</a>
Yang, Yezhou, et al. “Robot learning manipulation action plans by” watching” unconstrained videos from the world wide web.” Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 29. No. 1. 2015.</p>

<p><a id="c9">[9]</a>
Xiong, Caiming, et al. “Robot learning with a spatial, temporal, and causal and-or graph.” 2016 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2016.</p>

<p><a id="c10">[10]</a>
Sieb, Maximilian, et al. “Graph-structured visual imitation.” Conference on Robot Learning. PMLR, 2020.</p>

<p><a id="c11">[11]</a>
Paulius, David, and Yu Sun. “A survey of knowledge representation in service robotics.” Robotics and Autonomous Systems 118 (2019): 13-30.</p>

<p><a id="c12">[12]</a>
Chaumette, François. “Visual servoing using image features defined upon geometrical primitives.” Proceedings of 1994 33rd IEEE Conference on Decision and Control. Vol. 4. IEEE, 1994.</p>

<p><a id="c13">[13]</a>
Dodds, Zachary, et al. “Task specification and monitoring for uncalibrated hand/eye coordination.” Proceedings 1999 IEEE International Conference on Robotics and Automation (Cat. No. 99CH36288C). Vol. 2. IEEE, 1999.</p>

<p><a id="c14">[14]</a>
Hager, Gregory D., and Zachary Dodds. “On specifying and performing visual tasks with qualitative object models.” Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No. 00CH37065). Vol. 1. IEEE, 2000.</p>

<p><a id="c15">[15]</a>
Hespanha, João Pedro, et al. “What tasks can be performed with an uncalibrated stereo vision system?.” International Journal of Computer Vision 35.1 (1999): 65-85.</p>

<p><a id="c16">[16]</a>
Dodds, Zachary, et al. “A hierarchical vision architecture for robotic manipulation tasks.” International Conference on Computer Vision Systems. Springer, Berlin, Heidelberg, 1999.</p>

<p><a id="c17">[17]</a>
UCSD. Sapien project, “https://sapien.ucsd.edu/”, 2020.</p>

<p><a id="c18">[18]</a>
OCRTOC. Open cloud robot table organization challenge (ocrtoc), “http://www.ocrtoc.org/”, 2020.</p>

<p><a id="c19">[19]</a>
Hutchinson, Seth, Gregory D. Hager, and Peter I. Corke. “A tutorial on visual servo control.” IEEE transactions on robotics and automation 12.5 (1996): 651-670.</p>

<p><a id="c20">[20]</a>
Burke, Christopher J., et al. “Neural mechanisms of observational learning.” Proceedings of the National Academy of Sciences 107.32 (2010): 14431-14436.</p>

  </div>

</article>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

<!--    <h2 class="footer-heading">Blog</h2>-->

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
<!--          <li>Blog</li>-->
          <li><a href="mailto:jjin5@ualberta.ca">jjin5@ualberta.ca</a></li>
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/atlas-jj"><span class="icon icon--github"><svg viewBox="0 0 16 16"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">atlas-jj</span></a>

          </li>
          

          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>Welcome! This site is under construction.
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
