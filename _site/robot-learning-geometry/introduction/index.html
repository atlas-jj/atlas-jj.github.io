<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Introduction</title>
  <meta name="description" content="Welcome to our site!
">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="stylesheet" href="/css/custom.css">
  <link rel="canonical" href="http://yourdomain.com/robot-learning-geometry/introduction/">
  <link rel="alternate" type="application/rss+xml" title="Robot Learning Geometry" href="http://yourdomain.com/feed.xml">
  <!-- for mathjax support -->
    
      <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
        TeX: { equationNumbers: { autoNumber: "AMS" } }
        });
      </script>
      <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/robot-learning-geometry/">Robot Learning Geometry</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
        
          
          <a class="page-link" href="/robot-learning-geometry/">Cover Page</a>
          
        
          
          <a class="page-link" href="/robot-learning-geometry/introduction/">Introduction</a>
          
        
          
          <a class="page-link" href="/robot-learning-geometry/tutorial/">Tutorial</a>
          
        
          
          <a class="page-link" href="/robot-learning-geometry/proposal/">Proposal</a>
          
        
          
        
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="page-title-custom">Learning Geometry and Spatial Sense from Vision for Robotic Manipulation</h1>
  </header>

  <div class="post-content">
    <div style="text-align:center;">
<a href="/robot-learning-geometry/">Cover Page</a> &nbsp;| &nbsp;  <a class="active_href" href="/robot-learning-geometry/introduction/">Introduction</a> &nbsp; | &nbsp;  <a href="/robot-learning-geometry/tutorial">Tutorial</a>&nbsp;| &nbsp;  <a href="/robot-learning-geometry/proposal">Proposal</a>
</div>
<h6 class="page-author">Jun Jin <br /> Robotics &amp; Vision Lab, Computing Science, University of Alberta</h6>

<p><br /></p>

<blockquote>
  <p>'’Learn how to see. Realize that everything connects to everything else.” <br /> — Leonardo da Vinci</p>
</blockquote>

<p><br /></p>

<h2 id="1-introduction">1. Introduction</h2>
<p>One characteristic of robotic task learning that differs from computer vision or natural language processing, 
is that defining <em>“what is the task”</em> per se is typically more difficult than task objectives like object detection or text classification.</p>

<p>In fact, we have very limited options in our toolbox to define “<em>what</em>”, 
including task function in early works <a href="#c1">[1]</a><a href="#c2">[2]</a>, reward function in reinforcement learning <a href="#c3">[3]</a><a href="#c4">[4]</a>, 
behavior cloning in imitation learning <a href="#c5">[5]</a>, 
and hierarchical approaches like inferring task goals in zero-shot imitation learning <a href="#c6">[6]</a>, task planning in
neural task programmers <a href="#c7">[7]</a> and so on <a href="#c8">[8]</a><a href="#c9">[9]</a><a href="#c10">[10]</a>. 
Unfortunately, none of these have a thorough study on their capability to define “<em>what</em>”, 
nor enough attention has been given to the problem of task specification.</p>

<h4 id="11-robotic-task-representation-learning"><strong>1.1 Robotic Task Representation Learning</strong></h4>

<p><img src="/assets/images/overview2.png" alt="overview2" class="center" style="width:90%;" /></p>

<p>The goal of the research presented is to use structured priors and representation learning to encode <em>what</em> is a task, 
with which a task can be generally parameterized, monitored and controlled. 
This problem, which we call <strong>robotic task representation learning</strong> that involves studying</p>
<ul>
  <li>(1) What structured priors should be introduced to represent a task that enables general task specification.</li>
  <li>(2) How a task-relevant representation should be structured
and learned from raw image inputs.</li>
  <li>(3) How such representation enables easier controller design, provides task monitoring.</li>
  <li>(4) How representational invariance further enables generalizable robot learning across different environments and categorical objects.</li>
</ul>

<p>For example, by introducing a geometry structured task prior, 
a screwing task’s representation encoding <script type="math/tex">Z_{t}</script> stays consistent regardless of types of screw driver or screws since the task can be invariantly 
defined as a relationship between tooltip and the screw top.</p>

<p><img src="/assets/images/human_task_specification.png" alt="human_task_specification" class="center" /></p>

<p>Human empirically has a good sense of “what is the task”, as studied in observational learning<a href="#c11">[11]</a> in psychology, 
the rapid grasp of task concept by simply watching adults or peers during a human infant’s cognitive developmental process is astonishing. 
Generally, there are two types of structures for task specification used by humans:</p>
<ul>
  <li>(1) A semantic structure that conceptually describes how a task should be organized, for example a recipe defines how to make a pizza.</li>
  <li>(2) A geometric structure that solidly defines spatial information of a task, for example a tenon assembly task without any nails.</li>
</ul>

<p>Similarly, the above two structures are applied in robotic research. The semantic approach is intensively studied in ontology and affordance based methods<a href="#c11">[11]</a> which describe a task in a tree of 
graph structure based on objects,  attributes and their functionalities. 
However, the mapping from semantic task concepts to robot actions without hand-engineered motion primitives remains challenging. 
The geometric approach<a href="#c12">[12]</a><a href="#c13">[13]</a><a href="#c14">[14]</a><a href="#c15">[15]</a>, as a comparison, defines a task by object spatial relationships, geometric constraints 
(point-coincidence, line-parallel, etc.) and their chaining to construct more complex tasks<a href="#c16">[16]</a>. 
Besides the advantage of task interpretability, it provides a way to monitor task execution and relates directly to 
robot control as studied in visual servoing<a href="#c17">[17]</a>. However, this approach is <strong>rarely studied</strong>.</p>

<h4 id="12-making-sense-of-vision-by-visual-feature-connections"><strong>1.2 Making Sense of Vision by Visual Feature Connections</strong></h4>

<p><img src="/assets/images/chapter_1_intro.png" alt="chapter_1_intro" class="center" /></p>

<p>Using geometry and spatial sense to define a manipulation task, is essentially using a 
predefined structure to make connections between objects and geometric features (object parts), 
in order to make sense of what has been seen. The above figure shows 17 examples of how 
geometry and spatial sense applies in various manipulation, gaming, autonomous driving 
and collision avoidance tasks.</p>

<p><strong>The theme of this research</strong> is to introduce geometry and spatial sense as a structured task representation in robot learning, for the purpose of improving 
state-of-art robotic task specification capabilities.</p>

<h2 id="2-task-specification-by-geometric-constraints">2 Task Specification by Geometric Constraints</h2>

<div>
<img src="/assets/images/chapter2_real_examples.png" alt="chapter2_real_examples" class="center" style="width:50%;" /> 
<h5 style="font-weight:bold;text-align:center;">Tasks defined using geometric constraints[[15]](#c15).</h5>
</div>

<p>In robotics, representing a task using connection between geometric features/primitives was firstly studied in 
visual servoing<a href="#c17">[17]</a>, where the connection is structured using geometric constraints. Research started with two-point 
coincidence and soon expanded to point-to-line, line-to-line, more geometric features (planes, conics, cylinars)<a href="#c12">[12]</a> and 
hierarchical structure linking<a href="#c16">[16]</a> of the geometric constraints. Visual servoing, formed as a closed loop feedback control, 
maps <em>what</em> defined by observed geometric constraints to robot actions since the geometric loss signal directly links to 
camera/robot motion, which is referred as the <strong>virtual linkage</strong>. Introducing geometry and spatial sense as a relational 
inductive bias in robot learning can encode the spatial characteristics of a task.</p>

<p><img src="/assets/images/chapter2_basis.png" alt="chapter2_basis" class="center" /></p>

<p>However, in conventional visual servoing, specifying a task is manually done by selecting visual features and their relationships, 
and by long-term trackers of each visual feature. There is no learnable structure as of task definition encoding. 
Revisiting the nature of visual servoing, we have the following observation:
<br /></p>

<blockquote>
  <p>'’Using such features enables to realize a large variety of robotic tasks. … In fact, it can be shown that such an ability relies on the knowledge of the spatio-temporal behavior of a visual data with respect to camera motion.” <br /> — Chaumette, F. <a href="#c12">[12]</a></p>
</blockquote>

<p><br />
, which means all the matters is the spatio-temporal behavior that relates visual observations, task definitions and camera motion in
a unifying way. From a task function perspective, the above observation indicates all that matters is the spatio-temporal structure of the 
associated task function. Such <strong>spatio-temporal structure should be consistently retrieved</strong> from observed images for the purpose of continuous 
control. The common failures (either lost-tracking or feature outside camera’s field of view) of conventional visual servoing can be explained 
by the violations of such consistency.</p>

<p>We propose to directly learn such spatio-temporal structure of the associated task function. Moreover, since the spatial part of such behavior 
can directly link to camera motions using either virtual linkage as described above or model based learning methods as described in Chapter 2, 
studying how the spatial part relates to the task definition becomes the dominant problem, which we format using <strong>robotic task representation learning</strong> as learning a 
<strong>task encoder</strong> that consistently retrieves a <strong>task encoding</strong> from an observed image given geometry and spatial sense as the <strong>structured priors</strong>.</p>

<p>Intuitively, there are three learning paradigms can be used to learn such task encoding:</p>

<ul>
  <li>(1) A supervised learning approach using human annotated samples, but requires intimidating word load for each task which is not practical.</li>
  <li>(2) An reinforcement learning (RL) approach using reward as task definition and geometry as inductive bias in the state representation, but requires numerous robot environment interactions which is not practical either.</li>
  <li>(3) An unsupervised learning approach using 3rd-view human demonstration video performing the task, but requires unsupervised clues to relate the structured task encoding to task definition.</li>
</ul>

<p><img src="/assets/images/Our_approach.png" alt="Our_approach" class="center" style="width: 90%;" /></p>

<p>In this research, we propose a series of unsupervised learning methods to directly learn this task encoding from 3rd view human demonstration videos. Through examples of various manipulation tasks, we prove introducing geometry structure in robot learning will largely reduce needed human demonstrations, improve learning efficiency, demonstrate better generalization performance.</p>

<h2 id="references">References</h2>
<p><a id="c1">[1]</a> 
Samson, Claude, Bernard Espiau, and Michel Le Borgne. Robot control: the task function approach. Oxford University Press, Inc., 1991.</p>

<p><a id="c2">[2]</a> 
Samson, C., and B. Espiau. “Application of the task-function approach to sensor-based control of robot manipulators.” IFAC Proceedings Volumes 23.8 (1990): 269-274.</p>

<p><a id="c3">[3]</a>
Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018.</p>

<p><a id="c4">[4]</a>
Levine, Sergey, et al. “End-to-end training of deep visuomotor policies.” The Journal of Machine Learning Research 17.1 (2016): 1334-1373.</p>

<p><a id="c5">[5]</a>
Argall, Brenna D., et al. “A survey of robot learning from demonstration.” Robotics and autonomous systems 57.5 (2009): 469-483.</p>

<p><a id="c6">[6]</a>
Pathak, Deepak, et al. “Zero-shot visual imitation.” Proceedings of the IEEE conference on computer vision and pattern recognition workshops. 2018.</p>

<p><a id="c7">[7]</a>
Xu, Danfei, et al. “Neural task programming: Learning to generalize across hierarchical tasks.” 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2018.</p>

<p><a id="c8">[8]</a>
Yang, Yezhou, et al. “Robot learning manipulation action plans by” watching” unconstrained videos from the world wide web.” Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 29. No. 1. 2015.</p>

<p><a id="c9">[9]</a>
Xiong, Caiming, et al. “Robot learning with a spatial, temporal, and causal and-or graph.” 2016 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2016.</p>

<p><a id="c10">[10]</a>
Sieb, Maximilian, et al. “Graph-structured visual imitation.” Conference on Robot Learning. PMLR, 2020.</p>

<p><a id="c11">[11]</a>
Burke, Christopher J., et al. “Neural mechanisms of observational learning.” Proceedings of the National Academy of Sciences 107.32 (2010): 14431-14436.</p>

<p><a id="c12">[12]</a>
Chaumette, François. “Visual servoing using image features defined upon geometrical primitives.” Proceedings of 1994 33rd IEEE Conference on Decision and Control. Vol. 4. IEEE, 1994.</p>

<p><a id="c13">[13]</a>
Dodds, Zachary, et al. “Task specification and monitoring for uncalibrated hand/eye coordination.” Proceedings 1999 IEEE International Conference on Robotics and Automation (Cat. No. 99CH36288C). Vol. 2. IEEE, 1999.</p>

<p><a id="c14">[14]</a>
Hager, Gregory D., and Zachary Dodds. “On specifying and performing visual tasks with qualitative object models.” Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No. 00CH37065). Vol. 1. IEEE, 2000.</p>

<p><a id="c15">[15]</a>
Hespanha, João Pedro, et al. “What tasks can be performed with an uncalibrated stereo vision system?.” International Journal of Computer Vision 35.1 (1999): 65-85.</p>

<p><a id="c16">[16]</a>
Dodds, Zachary, et al. “A hierarchical vision architecture for robotic manipulation tasks.” International Conference on Computer Vision Systems. Springer, Berlin, Heidelberg, 1999.</p>

<p><a id="c17">[17]</a>
Hutchinson, Seth, Gregory D. Hager, and Peter I. Corke. “A tutorial on visual servo control.” IEEE transactions on robotics and automation 12.5 (1996): 651-670.</p>

  </div>

</article>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

<!--    <h2 class="footer-heading">Robot Learning Geometry</h2>-->

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
<!--          <li>Robot Learning Geometry</li>-->
          <li><a href="mailto:jjin5@ualberta.ca">jjin5@ualberta.ca</a></li>
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/atlas-jj"><span class="icon icon--github"><svg viewBox="0 0 16 16"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">atlas-jj</span></a>

          </li>
          

          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>Welcome to our site!
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
